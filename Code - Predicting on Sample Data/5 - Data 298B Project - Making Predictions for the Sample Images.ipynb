{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zX3NNjKQsnsK"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import time\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import joblib\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from tensorflow.keras.applications.densenet import DenseNet121, preprocess_input as preprocess_input_densenet\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing import image"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P58YPVEyOW3i","executionInfo":{"status":"ok","timestamp":1711046950924,"user_tz":420,"elapsed":3238,"user":{"displayName":"Nassim Ali-Chaouche","userId":"15197430272343455773"}},"outputId":"b1a38f87-3d5a-4f1b-e726-edbb5addd087"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["base_dir = \"/content/drive/MyDrive/Data 298B Project Data/Test Dataset - Workbook 2\"\n","image_folder = f'{base_dir}/Sample Images'"],"metadata":{"id":"IotE9hCbOXhZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the label encoding and scaler from previously made joblib files\n","\n","label_encoder = joblib.load(f'{base_dir}/label_encoder_v2_hybrid_model.joblib')\n","scaler = joblib.load(f'{base_dir}/scaler.joblib')"],"metadata":{"id":"SiH0u8FtOcfZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading the hybrid DenseNet121 model\n","start_time = time.time()\n","\n","model = load_model(f'{base_dir}/Best_DenseNet121_Hybrid_Model.h5')\n","\n","elapsed_time = time.time() - start_time\n","print(f\"It took {elapsed_time:.4f} seconds to load the model.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CaDu71kUOfYT","executionInfo":{"status":"ok","timestamp":1711046957489,"user_tz":420,"elapsed":6111,"user":{"displayName":"Nassim Ali-Chaouche","userId":"15197430272343455773"}},"outputId":"3785403d-c3c7-4112-b435-3cd5abcdff27"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["It took 6.4737 seconds to load the model.\n"]}]},{"cell_type":"code","source":["# Loading the numerical data\n","\n","numerical_df = pd.read_csv(f\"{base_dir}/combined_data.csv\")"],"metadata":{"id":"Yadgb966On4T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Specifying which numerical features to standardize\n","features_to_standardize = ['Avg Temp 14d', 'Avg Humidity 14d', 'Total Precipitation 14d', 'Avg Wind Speed 14d']\n","# Specifying all the numerical features to use in the model\n","all_numerical_features = ['Avg Temp 14d', 'Avg Humidity 14d', 'Total Precipitation 14d', 'Avg Wind Speed 14d', 'NDVI MODIS', 'NDVI - 1 MODIS', 'NDVI - 2 MODIS',\n","       'EVI MODIS', 'EVI - 1 MODIS', 'EVI - 2 MODIS', 'NDVI 1 Decrease',\n","       'NDVI 2 Decrease', 'EVI 1 Decrease', 'EVI 2 Decrease']"],"metadata":{"id":"jmp2-nAhO1hO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardizing the specified numerical features using a previous joblib file\n","\n","if features_to_standardize:\n","    # Loading the scaler\n","    loaded_scaler = joblib.load(f'{base_dir}/scaler.joblib')\n","    # Transforming the sample numerical data using the loaded scaler\n","    numerical_df[features_to_standardize] = loaded_scaler.transform(numerical_df[features_to_standardize])"],"metadata":{"id":"Gx2woG4CPJIq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Functions to preprocess images based on the base DenseNet121 pre-trained model\n","\n","def preprocess_image_densenet121(image_path):\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = tf.image.resize_with_pad(image, 224, 224, antialias=True)\n","    image = preprocess_input_densenet(image)\n","    image = np.expand_dims(image, axis=0)\n","    return image"],"metadata":{"id":"U5EswxYRPXXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjusting the 'Id' column to include the full image path\n","numerical_df['Id'] = numerical_df['Id'].apply(lambda x: os.path.join(image_folder, x))"],"metadata":{"id":"8TsqDzi9QVeE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","# Generating predictions for each instance, and showing the confidence level of the predicted class\n","start_time = time.time()\n","\n","# Preparing columns in the dataframe for predictions\n","numerical_df['Class Confidence Levels'] = np.nan\n","numerical_df['Class Prediction'] = np.nan\n","\n","# Predicting and filling the dataframe with the predicted class and confidence levels\n","for index, row in numerical_df.iterrows():\n","    img_array = preprocess_image_densenet121(row['Id'])\n","    num_data = row[all_numerical_features].to_numpy().reshape(1, -1)\n","    num_data = np.array(num_data, dtype=np.float32)\n","\n","    # Generating class probability predictions\n","    prediction = model.predict([img_array, num_data])[0]\n","\n","    # Determining the predicted class and its confidence\n","    predicted_class_idx = np.argmax(prediction)\n","    predicted_class = label_encoder.inverse_transform([predicted_class_idx])[0]\n","    class_confidence = prediction[predicted_class_idx]\n","\n","    # Updating the DataFrame with the prediction and confidence\n","    numerical_df.at[index, 'Class Confidence Levels'] = f\"{predicted_class}: {class_confidence:.4f}\"\n","    numerical_df.at[index, 'Class Prediction'] = predicted_class\n","\n","elapsed_time = time.time() - start_time\n","print(f\"It took {elapsed_time:.4f} seconds to generate predictions for each instance.\")\n","'''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"cDallhKCVxBu","executionInfo":{"status":"ok","timestamp":1711046957491,"user_tz":420,"elapsed":8,"user":{"displayName":"Nassim Ali-Chaouche","userId":"15197430272343455773"}},"outputId":"e05f5825-ab66-4b38-f63f-13af6d6f21f5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# Generating predictions for each instance, and showing the confidence level of the predicted class\\nstart_time = time.time()\\n\\n# Preparing columns in the dataframe for predictions\\nnumerical_df[\\'Class Confidence Levels\\'] = np.nan\\nnumerical_df[\\'Class Prediction\\'] = np.nan\\n\\n# Predicting and filling the dataframe with the predicted class and confidence levels\\nfor index, row in numerical_df.iterrows():\\n    img_array = preprocess_image_densenet121(row[\\'Id\\'])\\n    num_data = row[all_numerical_features].to_numpy().reshape(1, -1)\\n    num_data = np.array(num_data, dtype=np.float32)\\n\\n    # Generating class probability predictions\\n    prediction = model.predict([img_array, num_data])[0]\\n\\n    # Determining the predicted class and its confidence\\n    predicted_class_idx = np.argmax(prediction)\\n    predicted_class = label_encoder.inverse_transform([predicted_class_idx])[0]\\n    class_confidence = prediction[predicted_class_idx]\\n\\n    # Updating the DataFrame with the prediction and confidence\\n    numerical_df.at[index, \\'Class Confidence Levels\\'] = f\"{predicted_class}: {class_confidence:.4f}\"\\n    numerical_df.at[index, \\'Class Prediction\\'] = predicted_class\\n\\nelapsed_time = time.time() - start_time\\nprint(f\"It took {elapsed_time:.4f} seconds to generate predictions for each instance.\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Generating predictions for each instance, and showing the confidence level of every class\n","\n","start_time = time.time()\n","\n","# Preparing columns in the dataframe for predictions\n","numerical_df['Class Confidence Levels'] = np.nan\n","numerical_df['Class Prediction'] = np.nan\n","\n","# Predicting and filling the dataframe with the predicted class and confidence levels\n","for index, row in numerical_df.iterrows():\n","    img_array = preprocess_image_densenet121(row['Id'])\n","    num_data = row[all_numerical_features].to_numpy().reshape(1, -1)\n","    num_data = np.array(num_data, dtype=np.float32)\n","\n","    # Generating class probability predictions\n","    prediction = model.predict([img_array, num_data])[0]\n","\n","    # Formatting the predicted confidence levels for all classes\n","    confidences = {label_encoder.classes_[i]: round(float(prediction[i]), 4) for i in range(len(prediction))}\n","\n","    # Sorting confidences so that the highest confidence is first\n","    sorted_confidences = dict(sorted(confidences.items(), key=lambda item: item[1], reverse=True))\n","\n","    # Determining the predicted class\n","    predicted_class = max(sorted_confidences, key=sorted_confidences.get)\n","\n","    # Updating the dataframe with the prediction and confidence levels\n","    numerical_df.at[index, 'Class Confidence Levels'] = str(sorted_confidences)\n","    numerical_df.at[index, 'Class Prediction'] = predicted_class\n","\n","elapsed_time = time.time() - start_time\n","print(f\"It took {elapsed_time:.4f} seconds to generate predictions for all of the instances.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3Iv2KYri_k2","executionInfo":{"status":"ok","timestamp":1711046966170,"user_tz":420,"elapsed":8686,"user":{"displayName":"Nassim Ali-Chaouche","userId":"15197430272343455773"}},"outputId":"e72be32c-b797-4764-c2bc-bc7ff8d77113"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 3s 3s/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 33ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 32ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 29ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 30ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 31ms/step\n","1/1 [==============================] - 0s 29ms/step\n","It took 8.6912 seconds to generate predictions for all of the instances.\n"]}]},{"cell_type":"code","source":["# Selecting specific columns to save\n","columns_to_save = ['Id', 'Latitude', 'Longitude', 'Date', 'Class Confidence Levels', 'Class Prediction']\n","export_df = numerical_df[columns_to_save]"],"metadata":{"id":"BunChODtj4Sl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving the selected columns to a new CSV file\n","export_df.to_csv(f'{base_dir}/predictions_with_confidences.csv', index=False)"],"metadata":{"id":"yVZx488uj5Ix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving the selected columns to a new JSON file\n","export_df.to_json(f'{base_dir}/predictions_with_confidences.json', orient='records')"],"metadata":{"id":"CKgU91gpnmCz"},"execution_count":null,"outputs":[]}]}