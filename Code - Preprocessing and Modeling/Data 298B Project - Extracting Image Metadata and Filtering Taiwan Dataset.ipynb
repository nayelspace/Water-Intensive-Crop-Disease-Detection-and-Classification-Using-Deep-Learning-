{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMCoLnqEH6hr/4E9qdzTiBZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Extracting metadata and creating a dataframe**"],"metadata":{"id":"BvGqEM2cIRdF"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"S0jcIt3sIiXm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708558785346,"user_tz":480,"elapsed":28027,"user":{"displayName":"Nassim Ali-Chaouche","userId":"15197430272343455773"}},"outputId":"a8fd70bf-4333-4e8c-f4cf-b5e676b46e0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install ImageHash"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kwyh5dfQDeXl","executionInfo":{"status":"ok","timestamp":1708559047417,"user_tz":480,"elapsed":7003,"user":{"displayName":"Nassim Ali-Chaouche","userId":"15197430272343455773"}},"outputId":"a3513d91-cd27-420b-e7bf-5c59da5393d8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ImageHash\n","  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/296.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/296.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from ImageHash) (1.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ImageHash) (1.25.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from ImageHash) (9.4.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from ImageHash) (1.11.4)\n","Installing collected packages: ImageHash\n","Successfully installed ImageHash-4.3.1\n"]}]},{"cell_type":"code","source":["# Function to create a dataframe with the metadata\n","\n","from PIL import Image\n","from PIL.ExifTags import TAGS, GPSTAGS\n","import os\n","import pandas as pd\n","import shutil\n","\n","def get_exif_data(image_path):\n","    \"\"\"Extracting EXIF data from an image.\"\"\"\n","    image = Image.open(image_path)\n","    exif_data = {}\n","\n","    if hasattr(image, '_getexif'):\n","        exif_info = image._getexif()\n","        if exif_info:\n","            for tag, value in exif_info.items():\n","                decoded = TAGS.get(tag, tag)\n","                exif_data[decoded] = value\n","\n","    return exif_data\n","\n","def get_gps_info(exif_data):\n","    \"\"\"Extracting the GPSInfo dict from EXIF data.\"\"\"\n","    for key, val in exif_data.items():\n","        if key == 'GPSInfo':\n","            gps_info = {}\n","            for t in val:\n","                sub_decoded = GPSTAGS.get(t, t)\n","                gps_info[sub_decoded] = val[t]\n","            return gps_info\n","    return None\n","\n","def gps_info_to_decimal(gps_info):\n","    \"\"\"Converting GPSInfo to decimal degrees for latitude and longitude.\"\"\"\n","    def convert_to_degrees(value):\n","        \"\"\"Converts GPS coordinates to decimal degrees.\"\"\"\n","        d, m, s = value\n","        return d + (m / 60.0) + (s / 3600.0)\n","\n","    if gps_info:\n","        lat = gps_info.get('GPSLatitude')\n","        lat_ref = gps_info.get('GPSLatitudeRef')\n","        lon = gps_info.get('GPSLongitude')\n","        lon_ref = gps_info.get('GPSLongitudeRef')\n","\n","        if lat and lat_ref and lon and lon_ref:\n","            lat_decimal = convert_to_degrees(lat)\n","            lon_decimal = convert_to_degrees(lon)\n","\n","            if lat_ref == 'S':\n","                lat_decimal = -lat_decimal\n","            if lon_ref == 'W':\n","                lon_decimal = -lon_decimal\n","\n","            return lat_decimal, lon_decimal\n","    return None, None\n","\n","\n","def extract_date_time(exif_data):\n","    \"\"\"Extracting the DateTime from EXIF data if available.\"\"\"\n","    if 'DateTime' in exif_data:\n","        return exif_data['DateTime']\n","    return None\n","\n","def process_images(folder_path):\n","    \"\"\"Processes all images in the specified folder and its subfolders.\"\"\"\n","    data = []\n","\n","    for root, dirs, files in os.walk(folder_path):\n","        label = os.path.basename(root)\n","        for file in files:\n","            if file.lower().endswith(('.jpg', '.jpeg')):\n","                try:\n","                    image_path = os.path.join(root, file)\n","                    exif_data = get_exif_data(image_path)\n","                    gps_info = get_gps_info(exif_data)\n","                    latitude, longitude = gps_info_to_decimal(gps_info) if gps_info else (None, None)\n","                    date_time = extract_date_time(exif_data)\n","                    data.append({\n","                        \"Id\": file,\n","                        \"Latitude\": latitude,\n","                        \"Longitude\": longitude,\n","                        \"Date and Time\": date_time,\n","                        \"Class\": label\n","                    })\n","                except Exception as e:\n","                    print(f\"Error processing {file}: {e}\")\n","\n","    return pd.DataFrame(data)\n","\n","# Showing an example usage\n","#folder_path = 'path_to_main_folder'  # Folder that has the subfolder(s) containing the images\n","#df = process_images(folder_path) # After this point, can run the following code to obtain a 'Date' column (used later)"],"metadata":{"id":"TPHJtjbY0rwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","# Adding a 'Date' column and exporting the df\n","\n","# Converting the datetime string to a pandas datetime object\n","df['Date'] = pd.to_datetime(df['Date and Time'], format='%Y:%m:%d %H:%M:%S')\n","\n","# Extracting the date part and assigning it to a new column\n","df['Date'] = df['Date'].dt.date\n","\n","# Converting the date column to a string type in the 'YYYY-MM-DD' format\n","df['Date'] = df['Date'].astype(str)\n","\n","df.to_csv(f\"{folder_path}/image_metadata.csv\", index=False)\n","'''"],"metadata":{"id":"RjROtGrGjeng"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Filtering the Taiwan Dataset (Removing Duplicates, Keeping Images with Location and Date Information)**"],"metadata":{"id":"hP3jJzuxkM9Y"}},{"cell_type":"code","source":["base_dir = \"/content/drive/MyDrive/Data 298B Project Data/Rice Image Datasets - with Location and Time\"\n","\n","new_path_taiwan = f\"{base_dir}/Rice Leaf Diseases - Taiwan Filtered\"\n","\n","if not os.path.exists(new_path_taiwan):\n","    os.makedirs(new_path_taiwan)"],"metadata":{"id":"NeapRP8lJrho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Taiwan Data\n","\n","folder_path_taiwan = f\"{base_dir}/Rice Leaf Diseases - Taiwan\"\n","df_taiwan = process_images(folder_path_taiwan)\n","\n","df_taiwan.to_csv(f\"{new_path_taiwan}/image_metadata_taiwan_full.csv\", index=False)"],"metadata":{"id":"mHdLwOaDGZci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Removing duplicates from the Taiwan data (and keeping the file with the shortest name)\n","\n","import os\n","import imagehash\n","from PIL import Image\n","\n","def find_and_delete_duplicates(root_folder):\n","    # Dictionary to store the image hash as the key and a tuple of image path and filename length as the value\n","    hash_dict = {}\n","\n","    for subdir, dirs, files in os.walk(root_folder):\n","        for filename in files:\n","            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n","                file_path = os.path.join(subdir, filename)\n","\n","                # Opening the image and calculating its hash\n","                with Image.open(file_path) as img:\n","                    img_hash = imagehash.average_hash(img)\n","\n","                    if img_hash in hash_dict:\n","                        existing_path, existing_name_length = hash_dict[img_hash]\n","\n","                        # Comparing filename lengths and deleting the longer one\n","                        if len(filename) < existing_name_length:\n","                            os.remove(existing_path)\n","                            # Updating the dictionary with the new, shorter filename\n","                            hash_dict[img_hash] = (file_path, len(filename))\n","                        else:\n","                            # Deleting the current file as its name is longer\n","                            os.remove(file_path)\n","                    else:\n","                        # Adding the new image hash to the dictionary\n","                        hash_dict[img_hash] = (file_path, len(filename))\n","\n","find_and_delete_duplicates(folder_path_taiwan) # Folder containing the subfolders with images"],"metadata":{"id":"ansR9NvhqcXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a new CSV with image metadata after duplicates are removed\n","\n","df_taiwan_no_duplicates = process_images(folder_path_taiwan)\n","df_taiwan_no_duplicates.to_csv(f\"{new_path_taiwan}/image_metadata_taiwan_no_duplicates.csv\", index=False)"],"metadata":{"id":"g4GdayyOtwXY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating a new CSV with image metadata after filtering for images that have a 'Latitude', 'Longitude', and 'Date and Time'\n","\n","filtered_df_taiwan = df_taiwan_no_duplicates.dropna(subset=['Latitude', 'Longitude', 'Date and Time'])"],"metadata":{"id":"q6GwlGgkqbmT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adding a 'Date' column and exporting the df\n","\n","# Converting the datetime string to a pandas datetime object\n","filtered_images_df_taiwan['Date'] = pd.to_datetime(filtered_images_df_taiwan['Date and Time'], format='%Y:%m:%d %H:%M:%S')\n","\n","# Extracting the date part and assigning it to a new column\n","filtered_images_df_taiwan['Date'] = filtered_images_df_taiwan['Date'].dt.date\n","\n","# Converting the date column to a string type in the 'YYYY-MM-DD' format\n","filtered_images_df_taiwan['Date'] = filtered_images_df_taiwan['Date'].astype(str)\n","\n","filtered_images_df_taiwan.to_csv(f\"{new_path_taiwan}/image_metadata_taiwan_filtered_location.csv\", index=False)"],"metadata":{"id":"pkGas3DwgTdQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Copying the images from the filtered Taiwan dataset to a new folder while maintaining the same subdirectory structure\n","\n","# Loading the filtered image metadata\n","df_filtered_taiwan = pd.read_csv(f\"{new_path_taiwan}/image_metadata_taiwan_filtered_location.csv\")\n","\n","original_base_path = f\"{base_dir}/Rice Leaf Diseases - Taiwan\"\n","new_base_path = f\"{base_dir}/Rice Leaf Diseases - Taiwan Filtered\"\n","\n","if not os.path.exists(new_base_path):\n","    os.makedirs(new_base_path)\n","\n","# Copying the images\n","for _, row in df_filtered_taiwan.iterrows():\n","    original_subfolder_path = os.path.join(original_base_path, row['Class'])\n","    new_subfolder_path = os.path.join(new_base_path, row['Class'])\n","\n","    if not os.path.exists(new_subfolder_path):\n","        os.makedirs(new_subfolder_path)\n","\n","    source_path = os.path.join(original_subfolder_path, row['Id'])\n","    destination_path = os.path.join(new_subfolder_path, row['Id'])\n","\n","    shutil.copy2(source_path, destination_path)\n","    #print(f\"Copied: {source_path} to {destination_path}\")\n","\n","print(\"All images have been copied.\")\n"],"metadata":{"id":"M5zR-R-XLQ__","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708559506908,"user_tz":480,"elapsed":32762,"user":{"displayName":"Nassim Ali-Chaouche","userId":"15197430272343455773"}},"outputId":"fbcf58cc-689b-421c-914d-c7788496fa23"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All images have been copied.\n"]}]}]}